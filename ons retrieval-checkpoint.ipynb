{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import print_function, division, absolute_import, unicode_literals\n",
      "from six.moves import zip\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib2\n",
      "import json\n",
      "\n",
      "def geturl(url):\n",
      "    p = urllib2.urlopen(url)\n",
      "    page = p.read()\n",
      "    return page"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "LINK_PREFIX, NEXT_PREFIX = 'http://www.ons.gov.uk', 'http://www.ons.gov.uk/ons/datasets-and-tables/index.html'\n",
      "\n",
      "def page_to_links(page):\n",
      "    \"takes an ONS search result page and extracts the links to datasets and the next result\"\n",
      "    soup = BeautifulSoup(page)\n",
      "    link_tds = (td for td in soup.find_all('td') if td.has_key('class'))\n",
      "    links = {(LINK_PREFIX + td.a['href']) : td.text \n",
      "               for td in link_tds}\n",
      "    assert 0 < len(links) <= 100\n",
      "    \n",
      "    nextrefs = {a['href'] for a in soup.find_all('a') if a.string == 'Next'}\n",
      "    nextpage = None\n",
      "    if len(nextrefs) == 1:\n",
      "        nextpage = NEXT_PREFIX + nextrefs.pop()\n",
      "    elif len(nextrefs) != 0:\n",
      "        raise Exception(\"Too many choices to resolve reference to next page\")\n",
      "    \n",
      "    return links, nextpage\n",
      "\n",
      "def fetch_query(url):\n",
      "    \"retrieve all the links from the ONS query at <url>\"    \n",
      "    page = geturl(url)\n",
      "    links, nextpage = {}, None\n",
      "    try:\n",
      "        links, nextpage = page_to_links(page)\n",
      "    except Exception as e:\n",
      "        print(\"page_to_links failed unexpectedly:\" + str(e))\n",
      "    \n",
      "    if nextpage:    \n",
      "        nextlinks = fetch_query(nextpage)\n",
      "        links.update(nextlinks)\n",
      "        \n",
      "    return links\n",
      "\n",
      "\n",
      "def fetch_all_links(fname=None):\n",
      "    \"retrieve the links to all datasets on the ONS site and save them to file <fname>\"\n",
      "    ECON_LINKS = 'http://www.ons.gov.uk/ons/datasets-and-tables/index.html?content-type=Dataset&nscl=Economy&pubdateRangeType=allDates&sortBy=pubdate&sortDirection=DESCENDING&newquery=*&pageSize=100&applyFilters=true&content-type-orig=%22Dataset%22+OR+content-type_original%3A%22Reference+table%22&content-type=Dataset'\n",
      "    OTHER_LINKS = 'http://www.ons.gov.uk/ons/datasets-and-tables/index.html?content-type=Dataset&nscl=Agriculture+and+Environment&nscl=Business+and+Energy&nscl=Children%2C+Education+and+Skills&nscl=Crime+and+Justice&nscl=Government&nscl=Health+and+Social+Care&nscl=Labour+Market&nscl=People+and+Places&nscl=Population&nscl=Travel+and+Transport&pubdateRangeType=allDates&sortBy=pubdate&sortDirection=DESCENDING&newquery=*&pageSize=100&applyFilters=true&content-type-orig=%22Dataset%22+OR+content-type_original%3A%22Reference+table%22'\n",
      "    \n",
      "    econ = fetch_query(ECON_LINKS)     # the query has to be split because the server\n",
      "    links = fetch_query(OTHER_LINKS)   # limits responses to 500 links\n",
      "    links.update(econ)\n",
      "    if fname:\n",
      "        with open(fname, 'w') as out:\n",
      "            out.write(json.dumps(links, indent=4))\n",
      "    return links\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def local_fetch_links(f='onsdata/onslinks.txt'):\n",
      "    'retrieve list of links from local json data in <f>'\n",
      "    with open(f) as onslinks:\n",
      "        jsonstring = ''.join(line for line in onslinks.readlines())\n",
      "        return json.loads(jsonstring)\n",
      "    raise Exception('Failed to load file %s' % f)\n",
      "\n",
      "DATAPAGE_REL = 'http://www.ons.gov.uk'\n",
      "\n",
      "def onsdata_filelinks(page):\n",
      "    \"takes html of an ONS dataset page; extracts title, date and links to the xls/csv files\"\n",
      "    soup = BeautifulSoup(page)\n",
      "    links = soup.find_all('a')\n",
      "    \n",
      "    title = soup.find_all('title')[0].text.strip()\n",
      "    \n",
      "    xls, csv = [DATAPAGE_REL + link['href'] \n",
      "                  for link in links \n",
      "                    if link.text in ('XLS format', 'CSV format')]\n",
      "        \n",
      "    tds = iter(soup.find_all('td'))\n",
      "    while tds.next().text != 'Published date': \n",
      "        pass\n",
      "    date = tds.next().text\n",
      "    \n",
      "    return title, date, xls, csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def links_to_filelinks(links):\n",
      "    \"\"\"take a link to an ONS dataset page and extract links to the \n",
      "    csv and xls files on the page\"\"\"\n",
      "    flinks = []\n",
      "    if not links: return []\n",
      "    \n",
      "    for link in links:\n",
      "        try:\n",
      "            page = geturl(link)\n",
      "            flinks.append(onsdata_filelinks(page))\n",
      "        except:\n",
      "            print(\"Failed to get file links for \" + link)\n",
      "    if flinks:\n",
      "        return flinks\n",
      "    raise Exception(\"No values found\")\n",
      "\n",
      "def fetch_flinks(title, date, xls, csv):\n",
      "    \"\"\"used by get_world to save the data files from the ONS\n",
      "    dataset page\"\"\"\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_world():\n",
      "    \"\"\"Get every ONS dataset in both csv and xls form from\n",
      "    a query to their web search interface. These are saved\n",
      "    for further processing; see HTMLParsing.py\"\"\"\n",
      "    pages = fetch_all_links('onsdata/onslinks.txt')\n",
      "    flinks = links_to_filelinks(pages.keys())\n",
      "    \n",
      "    with open(\"onsdata/onsfilelinks.txt\", 'w') as f:\n",
      "        f.write(json.dumps(flinks))\n",
      "        \n",
      "    for link in links:\n",
      "        try:\n",
      "            title, _, xls, csv = link\n",
      "            xlsf, csvf = geturl(xls), geturl(csv)\n",
      "            with open('onsdata/' + title + '.xls', 'w') as f:\n",
      "                f.write(xlsf)\n",
      "            with open('onsdata/' + title + '.csv', 'w') as f:\n",
      "                f.write(csvf)\n",
      "        except:\n",
      "            print(\"Couldn't retrieve pages for page \" + link[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}